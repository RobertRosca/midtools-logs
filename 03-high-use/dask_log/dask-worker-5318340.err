distributed.nanny - INFO -         Start Nanny at: 'tcp://10.255.34.150:33013'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.255.34.150:44766'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.255.34.150:40910'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.255.34.150:38961'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.255.34.150:34709'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.255.34.150:36384'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.255.34.150:44230'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.255.34.150:35816'
distributed.worker - INFO -       Start worker at:  tcp://10.255.34.150:32941
distributed.worker - INFO -       Start worker at:  tcp://10.255.34.150:45761
distributed.worker - INFO -          Listening to:  tcp://10.255.34.150:32941
distributed.worker - INFO -       Start worker at:  tcp://10.255.34.150:44701
distributed.worker - INFO -          Listening to:  tcp://10.255.34.150:45761
distributed.worker - INFO -          dashboard at:        10.255.34.150:45566
distributed.worker - INFO -          Listening to:  tcp://10.255.34.150:44701
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO -       Start worker at:  tcp://10.255.34.150:46870
distributed.worker - INFO -          dashboard at:        10.255.34.150:42600
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://10.255.34.150:36561
distributed.worker - INFO -       Start worker at:  tcp://10.255.34.150:33911
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO -          dashboard at:        10.255.34.150:34614
distributed.worker - INFO -       Start worker at:  tcp://10.255.34.150:36459
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://10.255.34.150:36561
distributed.worker - INFO -          Listening to:  tcp://10.255.34.150:46870
distributed.worker - INFO -          Listening to:  tcp://10.255.34.150:33911
distributed.worker - INFO -          dashboard at:        10.255.34.150:36997
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO -          dashboard at:        10.255.34.150:38510
distributed.worker - INFO -          dashboard at:        10.255.34.150:43559
distributed.worker - INFO -       Start worker at:  tcp://10.255.34.150:33099
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO -          Listening to:  tcp://10.255.34.150:36459
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO -          dashboard at:        10.255.34.150:36474
distributed.worker - INFO -          Listening to:  tcp://10.255.34.150:33099
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        10.255.34.150:43228
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                   96.00 GB
distributed.worker - INFO -       Local Directory: /scratch/dask-worker-space/worker-byohc7g_
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   96.00 GB
distributed.worker - INFO -       Local Directory: /scratch/dask-worker-space/worker-4mrscgqa
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                   96.00 GB
distributed.worker - INFO -                Memory:                   96.00 GB
distributed.worker - INFO -       Local Directory: /scratch/dask-worker-space/worker-okj04sfd
distributed.worker - INFO -                Memory:                   96.00 GB
distributed.worker - INFO -                Memory:                   96.00 GB
distributed.worker - INFO -       Local Directory: /scratch/dask-worker-space/worker-tmi3t3cs
distributed.worker - INFO -       Local Directory: /scratch/dask-worker-space/worker-8tivjy3z
distributed.worker - INFO -                Memory:                   96.00 GB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/dask-worker-space/worker-lqvycwl7
distributed.worker - INFO -       Local Directory: /scratch/dask-worker-space/worker-nd3nge6v
distributed.worker - INFO -                Memory:                   96.00 GB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/dask-worker-space/worker-sqp2jxw2
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
Task exception was never retrieved
future: <Task finished coro=<connect.<locals>._() done, defined at /gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py:279> exception=CommClosedError()>
Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 288, in _
    handshake = await asyncio.wait_for(comm.read(), 1)
  File "/usr/lib64/python3.6/asyncio/tasks.py", line 351, in wait_for
    yield from waiter
concurrent.futures._base.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 295, in _
    raise CommClosedError()
distributed.comm.core.CommClosedError
Task exception was never retrieved
future: <Task finished coro=<connect.<locals>._() done, defined at /gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py:279> exception=CommClosedError()>
Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 288, in _
    handshake = await asyncio.wait_for(comm.read(), 1)
  File "/usr/lib64/python3.6/asyncio/tasks.py", line 351, in wait_for
    yield from waiter
concurrent.futures._base.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 295, in _
    raise CommClosedError()
distributed.comm.core.CommClosedError
Task exception was never retrieved
future: <Task finished coro=<connect.<locals>._() done, defined at /gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py:279> exception=CommClosedError()>
Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 288, in _
    handshake = await asyncio.wait_for(comm.read(), 1)
  File "/usr/lib64/python3.6/asyncio/tasks.py", line 351, in wait_for
    yield from waiter
concurrent.futures._base.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 295, in _
    raise CommClosedError()
distributed.comm.core.CommClosedError
Task exception was never retrieved
future: <Task finished coro=<connect.<locals>._() done, defined at /gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py:279> exception=CommClosedError()>
Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 288, in _
    handshake = await asyncio.wait_for(comm.read(), 1)
  File "/usr/lib64/python3.6/asyncio/tasks.py", line 351, in wait_for
    yield from waiter
concurrent.futures._base.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 295, in _
    raise CommClosedError()
distributed.comm.core.CommClosedError
Task exception was never retrieved
future: <Task finished coro=<connect.<locals>._() done, defined at /gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py:279> exception=CommClosedError()>
Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 288, in _
    handshake = await asyncio.wait_for(comm.read(), 1)
  File "/usr/lib64/python3.6/asyncio/tasks.py", line 351, in wait_for
    yield from waiter
concurrent.futures._base.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 295, in _
    raise CommClosedError()
distributed.comm.core.CommClosedError
Task exception was never retrieved
future: <Task finished coro=<connect.<locals>._() done, defined at /gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py:279> exception=CommClosedError()>
Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 288, in _
    handshake = await asyncio.wait_for(comm.read(), 1)
  File "/usr/lib64/python3.6/asyncio/tasks.py", line 351, in wait_for
    yield from waiter
concurrent.futures._base.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 295, in _
    raise CommClosedError()
distributed.comm.core.CommClosedError
Task exception was never retrieved
future: <Task finished coro=<connect.<locals>._() done, defined at /gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py:279> exception=CommClosedError()>
Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 288, in _
    handshake = await asyncio.wait_for(comm.read(), 1)
  File "/usr/lib64/python3.6/asyncio/tasks.py", line 351, in wait_for
    yield from waiter
concurrent.futures._base.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 295, in _
    raise CommClosedError()
distributed.comm.core.CommClosedError
Task exception was never retrieved
future: <Task finished coro=<connect.<locals>._() done, defined at /gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py:279> exception=CommClosedError()>
Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 288, in _
    handshake = await asyncio.wait_for(comm.read(), 1)
  File "/usr/lib64/python3.6/asyncio/tasks.py", line 351, in wait_for
    yield from waiter
concurrent.futures._base.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 295, in _
    raise CommClosedError()
distributed.comm.core.CommClosedError
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
Task exception was never retrieved
future: <Task finished coro=<connect.<locals>._() done, defined at /gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py:279> exception=CommClosedError()>
Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 288, in _
    handshake = await asyncio.wait_for(comm.read(), 1)
  File "/usr/lib64/python3.6/asyncio/tasks.py", line 351, in wait_for
    yield from waiter
concurrent.futures._base.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 295, in _
    raise CommClosedError()
distributed.comm.core.CommClosedError
Task exception was never retrieved
future: <Task finished coro=<connect.<locals>._() done, defined at /gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py:279> exception=CommClosedError()>
Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 288, in _
    handshake = await asyncio.wait_for(comm.read(), 1)
  File "/usr/lib64/python3.6/asyncio/tasks.py", line 351, in wait_for
    yield from waiter
concurrent.futures._base.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 295, in _
    raise CommClosedError()
distributed.comm.core.CommClosedError
Task exception was never retrieved
future: <Task finished coro=<connect.<locals>._() done, defined at /gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py:279> exception=CommClosedError()>
Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 288, in _
    handshake = await asyncio.wait_for(comm.read(), 1)
  File "/usr/lib64/python3.6/asyncio/tasks.py", line 351, in wait_for
    yield from waiter
concurrent.futures._base.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 295, in _
    raise CommClosedError()
distributed.comm.core.CommClosedError
Task exception was never retrieved
future: <Task finished coro=<connect.<locals>._() done, defined at /gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py:279> exception=CommClosedError()>
Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 288, in _
    handshake = await asyncio.wait_for(comm.read(), 1)
  File "/usr/lib64/python3.6/asyncio/tasks.py", line 351, in wait_for
    yield from waiter
concurrent.futures._base.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 295, in _
    raise CommClosedError()
distributed.comm.core.CommClosedError
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
Task exception was never retrieved
future: <Task finished coro=<connect.<locals>._() done, defined at /gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py:279> exception=CommClosedError()>
Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 288, in _
    handshake = await asyncio.wait_for(comm.read(), 1)
  File "/usr/lib64/python3.6/asyncio/tasks.py", line 351, in wait_for
    yield from waiter
concurrent.futures._base.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 295, in _
    raise CommClosedError()
distributed.comm.core.CommClosedError
Task exception was never retrieved
future: <Task finished coro=<connect.<locals>._() done, defined at /gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py:279> exception=CommClosedError()>
Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 288, in _
    handshake = await asyncio.wait_for(comm.read(), 1)
  File "/usr/lib64/python3.6/asyncio/tasks.py", line 351, in wait_for
    yield from waiter
concurrent.futures._base.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 295, in _
    raise CommClosedError()
distributed.comm.core.CommClosedError
Task exception was never retrieved
future: <Task finished coro=<connect.<locals>._() done, defined at /gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py:279> exception=CommClosedError()>
Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 288, in _
    handshake = await asyncio.wait_for(comm.read(), 1)
  File "/usr/lib64/python3.6/asyncio/tasks.py", line 351, in wait_for
    yield from waiter
concurrent.futures._base.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 295, in _
    raise CommClosedError()
distributed.comm.core.CommClosedError
Task exception was never retrieved
future: <Task finished coro=<connect.<locals>._() done, defined at /gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py:279> exception=CommClosedError()>
Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 288, in _
    handshake = await asyncio.wait_for(comm.read(), 1)
  File "/usr/lib64/python3.6/asyncio/tasks.py", line 351, in wait_for
    yield from waiter
concurrent.futures._base.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 295, in _
    raise CommClosedError()
distributed.comm.core.CommClosedError
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
Task exception was never retrieved
future: <Task finished coro=<connect.<locals>._() done, defined at /gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py:279> exception=CommClosedError()>
Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 288, in _
    handshake = await asyncio.wait_for(comm.read(), 1)
  File "/usr/lib64/python3.6/asyncio/tasks.py", line 351, in wait_for
    yield from waiter
concurrent.futures._base.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 295, in _
    raise CommClosedError()
distributed.comm.core.CommClosedError
Task exception was never retrieved
future: <Task finished coro=<connect.<locals>._() done, defined at /gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py:279> exception=CommClosedError()>
Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 288, in _
    handshake = await asyncio.wait_for(comm.read(), 1)
  File "/usr/lib64/python3.6/asyncio/tasks.py", line 351, in wait_for
    yield from waiter
concurrent.futures._base.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 295, in _
    raise CommClosedError()
distributed.comm.core.CommClosedError
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
Task exception was never retrieved
future: <Task finished coro=<connect.<locals>._() done, defined at /gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py:279> exception=CommClosedError()>
Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 288, in _
    handshake = await asyncio.wait_for(comm.read(), 1)
  File "/usr/lib64/python3.6/asyncio/tasks.py", line 351, in wait_for
    yield from waiter
concurrent.futures._base.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 295, in _
    raise CommClosedError()
distributed.comm.core.CommClosedError
Task exception was never retrieved
future: <Task finished coro=<connect.<locals>._() done, defined at /gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py:279> exception=CommClosedError()>
Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 288, in _
    handshake = await asyncio.wait_for(comm.read(), 1)
  File "/usr/lib64/python3.6/asyncio/tasks.py", line 351, in wait_for
    yield from waiter
concurrent.futures._base.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 295, in _
    raise CommClosedError()
distributed.comm.core.CommClosedError
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
Task exception was never retrieved
future: <Task finished coro=<connect.<locals>._() done, defined at /gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py:279> exception=CommClosedError()>
Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 288, in _
    handshake = await asyncio.wait_for(comm.read(), 1)
  File "/usr/lib64/python3.6/asyncio/tasks.py", line 351, in wait_for
    yield from waiter
concurrent.futures._base.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 295, in _
    raise CommClosedError()
distributed.comm.core.CommClosedError
Task exception was never retrieved
future: <Task finished coro=<connect.<locals>._() done, defined at /gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py:279> exception=CommClosedError()>
Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 288, in _
    handshake = await asyncio.wait_for(comm.read(), 1)
  File "/usr/lib64/python3.6/asyncio/tasks.py", line 351, in wait_for
    yield from waiter
concurrent.futures._base.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 295, in _
    raise CommClosedError()
distributed.comm.core.CommClosedError
Task exception was never retrieved
future: <Task finished coro=<connect.<locals>._() done, defined at /gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py:279> exception=CommClosedError()>
Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 288, in _
    handshake = await asyncio.wait_for(comm.read(), 1)
  File "/usr/lib64/python3.6/asyncio/tasks.py", line 351, in wait_for
    yield from waiter
concurrent.futures._base.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 295, in _
    raise CommClosedError()
distributed.comm.core.CommClosedError
Task exception was never retrieved
future: <Task finished coro=<connect.<locals>._() done, defined at /gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py:279> exception=CommClosedError()>
Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 288, in _
    handshake = await asyncio.wait_for(comm.read(), 1)
  File "/usr/lib64/python3.6/asyncio/tasks.py", line 351, in wait_for
    yield from waiter
concurrent.futures._base.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 295, in _
    raise CommClosedError()
distributed.comm.core.CommClosedError
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
Task exception was never retrieved
future: <Task finished coro=<connect.<locals>._() done, defined at /gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py:279> exception=CommClosedError()>
Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 288, in _
    handshake = await asyncio.wait_for(comm.read(), 1)
  File "/usr/lib64/python3.6/asyncio/tasks.py", line 351, in wait_for
    yield from waiter
concurrent.futures._base.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 295, in _
    raise CommClosedError()
distributed.comm.core.CommClosedError
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
Task exception was never retrieved
future: <Task finished coro=<connect.<locals>._() done, defined at /gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py:279> exception=CommClosedError()>
Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 288, in _
    handshake = await asyncio.wait_for(comm.read(), 1)
  File "/usr/lib64/python3.6/asyncio/tasks.py", line 351, in wait_for
    yield from waiter
concurrent.futures._base.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 295, in _
    raise CommClosedError()
distributed.comm.core.CommClosedError
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
Task exception was never retrieved
future: <Task finished coro=<connect.<locals>._() done, defined at /gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py:279> exception=CommClosedError()>
Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 288, in _
    handshake = await asyncio.wait_for(comm.read(), 1)
  File "/usr/lib64/python3.6/asyncio/tasks.py", line 351, in wait_for
    yield from waiter
concurrent.futures._base.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 295, in _
    raise CommClosedError()
distributed.comm.core.CommClosedError
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
Task exception was never retrieved
future: <Task finished coro=<connect.<locals>._() done, defined at /gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py:279> exception=CommClosedError()>
Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 288, in _
    handshake = await asyncio.wait_for(comm.read(), 1)
  File "/usr/lib64/python3.6/asyncio/tasks.py", line 351, in wait_for
    yield from waiter
concurrent.futures._base.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 295, in _
    raise CommClosedError()
distributed.comm.core.CommClosedError
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
Task exception was never retrieved
future: <Task finished coro=<connect.<locals>._() done, defined at /gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py:279> exception=CommClosedError()>
Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 288, in _
    handshake = await asyncio.wait_for(comm.read(), 1)
  File "/usr/lib64/python3.6/asyncio/tasks.py", line 351, in wait_for
    yield from waiter
concurrent.futures._base.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 295, in _
    raise CommClosedError()
distributed.comm.core.CommClosedError
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
Task exception was never retrieved
future: <Task finished coro=<connect.<locals>._() done, defined at /gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py:279> exception=CommClosedError()>
Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 288, in _
    handshake = await asyncio.wait_for(comm.read(), 1)
  File "/usr/lib64/python3.6/asyncio/tasks.py", line 351, in wait_for
    yield from waiter
concurrent.futures._base.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 295, in _
    raise CommClosedError()
distributed.comm.core.CommClosedError
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
Task exception was never retrieved
future: <Task finished coro=<connect.<locals>._() done, defined at /gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py:279> exception=CommClosedError()>
Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 288, in _
    handshake = await asyncio.wait_for(comm.read(), 1)
  File "/usr/lib64/python3.6/asyncio/tasks.py", line 351, in wait_for
    yield from waiter
concurrent.futures._base.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 295, in _
    raise CommClosedError()
distributed.comm.core.CommClosedError
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
Task exception was never retrieved
future: <Task finished coro=<connect.<locals>._() done, defined at /gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py:279> exception=CommClosedError()>
Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 288, in _
    handshake = await asyncio.wait_for(comm.read(), 1)
  File "/usr/lib64/python3.6/asyncio/tasks.py", line 351, in wait_for
    yield from waiter
concurrent.futures._base.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 295, in _
    raise CommClosedError()
distributed.comm.core.CommClosedError
Task exception was never retrieved
future: <Task finished coro=<connect.<locals>._() done, defined at /gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py:279> exception=CommClosedError()>
Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 288, in _
    handshake = await asyncio.wait_for(comm.read(), 1)
  File "/usr/lib64/python3.6/asyncio/tasks.py", line 351, in wait_for
    yield from waiter
concurrent.futures._base.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 295, in _
    raise CommClosedError()
distributed.comm.core.CommClosedError
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
Task exception was never retrieved
future: <Task finished coro=<connect.<locals>._() done, defined at /gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py:279> exception=CommClosedError()>
Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 288, in _
    handshake = await asyncio.wait_for(comm.read(), 1)
  File "/usr/lib64/python3.6/asyncio/tasks.py", line 351, in wait_for
    yield from waiter
concurrent.futures._base.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 295, in _
    raise CommClosedError()
distributed.comm.core.CommClosedError
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
Task exception was never retrieved
future: <Task finished coro=<connect.<locals>._() done, defined at /gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py:279> exception=CommClosedError()>
Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 288, in _
    handshake = await asyncio.wait_for(comm.read(), 1)
  File "/usr/lib64/python3.6/asyncio/tasks.py", line 351, in wait_for
    yield from waiter
concurrent.futures._base.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 295, in _
    raise CommClosedError()
distributed.comm.core.CommClosedError
Task exception was never retrieved
future: <Task finished coro=<connect.<locals>._() done, defined at /gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py:279> exception=CommClosedError()>
Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 288, in _
    handshake = await asyncio.wait_for(comm.read(), 1)
  File "/usr/lib64/python3.6/asyncio/tasks.py", line 351, in wait_for
    yield from waiter
concurrent.futures._base.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 295, in _
    raise CommClosedError()
distributed.comm.core.CommClosedError
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
Task exception was never retrieved
future: <Task finished coro=<connect.<locals>._() done, defined at /gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py:279> exception=CommClosedError()>
Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 288, in _
    handshake = await asyncio.wait_for(comm.read(), 1)
  File "/usr/lib64/python3.6/asyncio/tasks.py", line 351, in wait_for
    yield from waiter
concurrent.futures._base.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 295, in _
    raise CommClosedError()
distributed.comm.core.CommClosedError
Task exception was never retrieved
future: <Task finished coro=<connect.<locals>._() done, defined at /gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py:279> exception=CommClosedError()>
Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 288, in _
    handshake = await asyncio.wait_for(comm.read(), 1)
  File "/usr/lib64/python3.6/asyncio/tasks.py", line 351, in wait_for
    yield from waiter
concurrent.futures._base.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 295, in _
    raise CommClosedError()
distributed.comm.core.CommClosedError
Task exception was never retrieved
future: <Task finished coro=<connect.<locals>._() done, defined at /gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py:279> exception=CommClosedError()>
Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 288, in _
    handshake = await asyncio.wait_for(comm.read(), 1)
  File "/usr/lib64/python3.6/asyncio/tasks.py", line 351, in wait_for
    yield from waiter
concurrent.futures._base.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 295, in _
    raise CommClosedError()
distributed.comm.core.CommClosedError
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
Task exception was never retrieved
future: <Task finished coro=<connect.<locals>._() done, defined at /gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py:279> exception=CommClosedError()>
Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 288, in _
    handshake = await asyncio.wait_for(comm.read(), 1)
  File "/usr/lib64/python3.6/asyncio/tasks.py", line 351, in wait_for
    yield from waiter
concurrent.futures._base.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 295, in _
    raise CommClosedError()
distributed.comm.core.CommClosedError
Task exception was never retrieved
future: <Task finished coro=<connect.<locals>._() done, defined at /gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py:279> exception=CommClosedError()>
Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 288, in _
    handshake = await asyncio.wait_for(comm.read(), 1)
  File "/usr/lib64/python3.6/asyncio/tasks.py", line 351, in wait_for
    yield from waiter
concurrent.futures._base.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 295, in _
    raise CommClosedError()
distributed.comm.core.CommClosedError
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
Task exception was never retrieved
future: <Task finished coro=<connect.<locals>._() done, defined at /gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py:279> exception=CommClosedError()>
Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 288, in _
    handshake = await asyncio.wait_for(comm.read(), 1)
  File "/usr/lib64/python3.6/asyncio/tasks.py", line 351, in wait_for
    yield from waiter
concurrent.futures._base.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 295, in _
    raise CommClosedError()
distributed.comm.core.CommClosedError
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
Task exception was never retrieved
future: <Task finished coro=<connect.<locals>._() done, defined at /gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py:279> exception=CommClosedError()>
Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 288, in _
    handshake = await asyncio.wait_for(comm.read(), 1)
  File "/usr/lib64/python3.6/asyncio/tasks.py", line 351, in wait_for
    yield from waiter
concurrent.futures._base.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 295, in _
    raise CommClosedError()
distributed.comm.core.CommClosedError
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
Task exception was never retrieved
future: <Task finished coro=<connect.<locals>._() done, defined at /gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py:279> exception=CommClosedError()>
Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 288, in _
    handshake = await asyncio.wait_for(comm.read(), 1)
  File "/usr/lib64/python3.6/asyncio/tasks.py", line 351, in wait_for
    yield from waiter
concurrent.futures._base.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 295, in _
    raise CommClosedError()
distributed.comm.core.CommClosedError
Task exception was never retrieved
future: <Task finished coro=<connect.<locals>._() done, defined at /gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py:279> exception=CommClosedError()>
Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 288, in _
    handshake = await asyncio.wait_for(comm.read(), 1)
  File "/usr/lib64/python3.6/asyncio/tasks.py", line 351, in wait_for
    yield from waiter
concurrent.futures._base.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 295, in _
    raise CommClosedError()
distributed.comm.core.CommClosedError
Task exception was never retrieved
future: <Task finished coro=<connect.<locals>._() done, defined at /gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py:279> exception=CommClosedError()>
Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 288, in _
    handshake = await asyncio.wait_for(comm.read(), 1)
  File "/usr/lib64/python3.6/asyncio/tasks.py", line 351, in wait_for
    yield from waiter
concurrent.futures._base.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 295, in _
    raise CommClosedError()
distributed.comm.core.CommClosedError
Task exception was never retrieved
future: <Task finished coro=<connect.<locals>._() done, defined at /gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py:279> exception=CommClosedError()>
Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 288, in _
    handshake = await asyncio.wait_for(comm.read(), 1)
  File "/usr/lib64/python3.6/asyncio/tasks.py", line 351, in wait_for
    yield from waiter
concurrent.futures._base.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 295, in _
    raise CommClosedError()
distributed.comm.core.CommClosedError
Task exception was never retrieved
future: <Task finished coro=<connect.<locals>._() done, defined at /gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py:279> exception=CommClosedError()>
Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 288, in _
    handshake = await asyncio.wait_for(comm.read(), 1)
  File "/usr/lib64/python3.6/asyncio/tasks.py", line 351, in wait_for
    yield from waiter
concurrent.futures._base.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 295, in _
    raise CommClosedError()
distributed.comm.core.CommClosedError
Task exception was never retrieved
future: <Task finished coro=<connect.<locals>._() done, defined at /gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py:279> exception=CommClosedError()>
Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 288, in _
    handshake = await asyncio.wait_for(comm.read(), 1)
  File "/usr/lib64/python3.6/asyncio/tasks.py", line 351, in wait_for
    yield from waiter
concurrent.futures._base.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 295, in _
    raise CommClosedError()
distributed.comm.core.CommClosedError
Task exception was never retrieved
future: <Task finished coro=<connect.<locals>._() done, defined at /gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py:279> exception=CommClosedError()>
Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 288, in _
    handshake = await asyncio.wait_for(comm.read(), 1)
  File "/usr/lib64/python3.6/asyncio/tasks.py", line 351, in wait_for
    yield from waiter
concurrent.futures._base.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 295, in _
    raise CommClosedError()
distributed.comm.core.CommClosedError
Task exception was never retrieved
future: <Task finished coro=<connect.<locals>._() done, defined at /gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py:279> exception=CommClosedError()>
Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 288, in _
    handshake = await asyncio.wait_for(comm.read(), 1)
  File "/usr/lib64/python3.6/asyncio/tasks.py", line 351, in wait_for
    yield from waiter
concurrent.futures._base.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 295, in _
    raise CommClosedError()
distributed.comm.core.CommClosedError
distributed.worker - INFO -         Registered to:   tcp://10.255.32.50:36178
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.255.32.50:36178
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://10.255.32.50:36178
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.255.32.50:36178
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.255.32.50:36178
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.255.32.50:36178
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.255.32.50:36178
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.255.32.50:36178
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.worker - INFO - Stopping worker at tcp://10.255.34.150:32941
distributed.worker - INFO - Stopping worker at tcp://10.255.34.150:33099
distributed.worker - INFO - Stopping worker at tcp://10.255.34.150:33911
distributed.worker - INFO - Stopping worker at tcp://10.255.34.150:36459
distributed.worker - INFO - Stopping worker at tcp://10.255.34.150:44701
distributed.worker - INFO - Stopping worker at tcp://10.255.34.150:36561
distributed.worker - INFO - Stopping worker at tcp://10.255.34.150:46870
distributed.worker - INFO - Stopping worker at tcp://10.255.34.150:45761
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - WARNING - Restarting worker
distributed.nanny - WARNING - Restarting worker
distributed.worker - INFO -       Start worker at:  tcp://10.255.34.150:42102
distributed.worker - INFO -          Listening to:  tcp://10.255.34.150:42102
distributed.worker - INFO -          dashboard at:        10.255.34.150:38912
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   96.00 GB
distributed.worker - INFO -       Local Directory: /scratch/dask-worker-space/worker-7r9lhgki
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://10.255.34.150:33727
distributed.worker - INFO -          Listening to:  tcp://10.255.34.150:33727
distributed.worker - INFO -          dashboard at:        10.255.34.150:46268
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   96.00 GB
distributed.worker - INFO -       Local Directory: /scratch/dask-worker-space/worker-hq0qqns4
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://10.255.32.50:36178
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://10.255.34.150:37959
distributed.worker - INFO -          Listening to:  tcp://10.255.34.150:37959
distributed.worker - INFO -          dashboard at:        10.255.34.150:33477
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   96.00 GB
distributed.worker - INFO -       Local Directory: /scratch/dask-worker-space/worker-daqyqjdc
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://10.255.32.50:36178
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.255.32.50:36178
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:  tcp://10.255.34.150:40147
distributed.worker - INFO -          Listening to:  tcp://10.255.34.150:40147
distributed.worker - INFO -          dashboard at:        10.255.34.150:35925
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   96.00 GB
distributed.worker - INFO -       Local Directory: /scratch/dask-worker-space/worker-wqeuhd1j
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://10.255.34.150:38897
distributed.worker - INFO -          Listening to:  tcp://10.255.34.150:38897
distributed.worker - INFO -          dashboard at:        10.255.34.150:33882
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   96.00 GB
distributed.worker - INFO -       Local Directory: /scratch/dask-worker-space/worker-xmm3_c2r
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://10.255.32.50:36178
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.255.32.50:36178
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:  tcp://10.255.34.150:36997
distributed.worker - INFO -          Listening to:  tcp://10.255.34.150:36997
distributed.worker - INFO -          dashboard at:        10.255.34.150:33320
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   96.00 GB
distributed.worker - INFO -       Local Directory: /scratch/dask-worker-space/worker-xjskuh1t
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://10.255.34.150:42357
distributed.worker - INFO -          Listening to:  tcp://10.255.34.150:42357
distributed.worker - INFO -          dashboard at:        10.255.34.150:37105
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   96.00 GB
distributed.worker - INFO -       Local Directory: /scratch/dask-worker-space/worker-fc0guj4l
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://10.255.34.150:35539
distributed.worker - INFO -          Listening to:  tcp://10.255.34.150:35539
distributed.worker - INFO -          dashboard at:        10.255.34.150:37753
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   96.00 GB
distributed.worker - INFO -       Local Directory: /scratch/dask-worker-space/worker-885r8_h0
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://10.255.32.50:36178
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.255.32.50:36178
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.255.32.50:36178
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.worker - INFO - Stopping worker at tcp://10.255.34.150:33727
distributed.worker - INFO - Stopping worker at tcp://10.255.34.150:38897
distributed.worker - INFO - Stopping worker at tcp://10.255.34.150:36997
distributed.worker - INFO - Stopping worker at tcp://10.255.34.150:37959
distributed.worker - INFO - Stopping worker at tcp://10.255.34.150:35539
distributed.worker - INFO - Stopping worker at tcp://10.255.34.150:42102
distributed.worker - INFO - Stopping worker at tcp://10.255.34.150:42357
distributed.worker - INFO - Stopping worker at tcp://10.255.34.150:40147
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.worker - INFO -       Start worker at:  tcp://10.255.34.150:32936
distributed.worker - INFO -          Listening to:  tcp://10.255.34.150:32936
distributed.worker - INFO -          dashboard at:        10.255.34.150:35653
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   96.00 GB
distributed.worker - INFO -       Local Directory: /scratch/dask-worker-space/worker-7as3ca2o
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://10.255.32.50:36178
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:  tcp://10.255.34.150:42967
distributed.worker - INFO -          Listening to:  tcp://10.255.34.150:42967
distributed.worker - INFO -          dashboard at:        10.255.34.150:36098
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   96.00 GB
distributed.worker - INFO -       Local Directory: /scratch/dask-worker-space/worker-dy4gfdfm
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://10.255.32.50:36178
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:  tcp://10.255.34.150:37894
distributed.worker - INFO -          Listening to:  tcp://10.255.34.150:37894
distributed.worker - INFO -          dashboard at:        10.255.34.150:39861
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   96.00 GB
distributed.worker - INFO -       Local Directory: /scratch/dask-worker-space/worker-k9tacli5
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://10.255.34.150:44808
distributed.worker - INFO -          Listening to:  tcp://10.255.34.150:44808
distributed.worker - INFO -          dashboard at:        10.255.34.150:42968
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   96.00 GB
distributed.worker - INFO -       Local Directory: /scratch/dask-worker-space/worker-xu_6avw2
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://10.255.32.50:36178
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:  tcp://10.255.34.150:33613
distributed.worker - INFO -          Listening to:  tcp://10.255.34.150:33613
distributed.worker - INFO -          dashboard at:        10.255.34.150:42453
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   96.00 GB
distributed.worker - INFO -       Local Directory: /scratch/dask-worker-space/worker-hzkwe2nf
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://10.255.34.150:33333
distributed.worker - INFO -          Listening to:  tcp://10.255.34.150:33333
distributed.worker - INFO -          dashboard at:        10.255.34.150:43816
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   96.00 GB
distributed.worker - INFO -       Local Directory: /scratch/dask-worker-space/worker-ymck38v2
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://10.255.34.150:46260
distributed.worker - INFO -          Listening to:  tcp://10.255.34.150:46260
distributed.worker - INFO -          dashboard at:        10.255.34.150:37940
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   96.00 GB
distributed.worker - INFO -       Local Directory: /scratch/dask-worker-space/worker-wz9po9im
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://10.255.32.50:36178
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.255.32.50:36178
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.255.32.50:36178
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.255.32.50:36178
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://10.255.34.150:43928
distributed.worker - INFO -          Listening to:  tcp://10.255.34.150:43928
distributed.worker - INFO -          dashboard at:        10.255.34.150:36593
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   96.00 GB
distributed.worker - INFO -       Local Directory: /scratch/dask-worker-space/worker-85s3p4fj
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://10.255.32.50:36178
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
slurmstepd: error: *** JOB 5318340 ON max-exfl224 CANCELLED AT 2020-08-19T17:25:21 ***
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://10.255.34.150:44766'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.255.34.150:34709'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.255.34.150:36384'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.255.34.150:40910'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.255.34.150:44230'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.255.34.150:33013'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.255.34.150:38961'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.255.34.150:35816'
distributed.nanny - INFO - Worker process 34505 was killed by signal 15
distributed.nanny - INFO - Worker process 34506 was killed by signal 15
distributed.nanny - INFO - Worker process 34438 was killed by signal 15
