distributed.nanny - INFO -         Start Nanny at: 'tcp://10.255.34.151:46482'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.255.34.151:45741'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.255.34.151:46379'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.255.34.151:38024'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.255.34.151:33263'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.255.34.151:35774'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.255.34.151:45786'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.255.34.151:46243'
distributed.worker - INFO -       Start worker at:  tcp://10.255.34.151:37984
distributed.worker - INFO -          Listening to:  tcp://10.255.34.151:37984
distributed.worker - INFO -          dashboard at:        10.255.34.151:46234
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO -       Start worker at:  tcp://10.255.34.151:34969
distributed.worker - INFO -       Start worker at:  tcp://10.255.34.151:33841
distributed.worker - INFO -          Listening to:  tcp://10.255.34.151:34969
distributed.worker - INFO -          dashboard at:        10.255.34.151:35981
distributed.worker - INFO -          Listening to:  tcp://10.255.34.151:33841
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO -          dashboard at:        10.255.34.151:37989
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO -       Start worker at:  tcp://10.255.34.151:34382
distributed.worker - INFO -          Listening to:  tcp://10.255.34.151:34382
distributed.worker - INFO -          dashboard at:        10.255.34.151:32773
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO -       Start worker at:  tcp://10.255.34.151:43460
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://10.255.34.151:36769
distributed.worker - INFO -          Listening to:  tcp://10.255.34.151:43460
distributed.worker - INFO -          dashboard at:        10.255.34.151:41544
distributed.worker - INFO -          Listening to:  tcp://10.255.34.151:36769
distributed.worker - INFO -       Start worker at:  tcp://10.255.34.151:35910
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO -          dashboard at:        10.255.34.151:35593
distributed.worker - INFO -       Start worker at:  tcp://10.255.34.151:32927
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://10.255.34.151:35910
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO -          Listening to:  tcp://10.255.34.151:32927
distributed.worker - INFO -          dashboard at:        10.255.34.151:43920
distributed.worker - INFO -          dashboard at:        10.255.34.151:44261
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   96.00 GB
distributed.worker - INFO -       Local Directory: /scratch/dask-worker-space/worker-u4ubym31
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   96.00 GB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/dask-worker-space/worker-nvif5wp9
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO -                Memory:                   96.00 GB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                   96.00 GB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/dask-worker-space/worker-vp5qob6w
distributed.worker - INFO -       Local Directory: /scratch/dask-worker-space/worker-fezg2hql
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                   96.00 GB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/dask-worker-space/worker-uj0x_0hz
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   96.00 GB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /scratch/dask-worker-space/worker-o7gujydd
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   96.00 GB
distributed.worker - INFO -                Memory:                   96.00 GB
distributed.worker - INFO -       Local Directory: /scratch/dask-worker-space/worker-tm0kxwps
distributed.worker - INFO -       Local Directory: /scratch/dask-worker-space/worker-wnu9_axx
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
Task exception was never retrieved
future: <Task finished coro=<connect.<locals>._() done, defined at /gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py:279> exception=CommClosedError()>
Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 288, in _
    handshake = await asyncio.wait_for(comm.read(), 1)
  File "/usr/lib64/python3.6/asyncio/tasks.py", line 351, in wait_for
    yield from waiter
concurrent.futures._base.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 295, in _
    raise CommClosedError()
distributed.comm.core.CommClosedError
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
Task exception was never retrieved
future: <Task finished coro=<connect.<locals>._() done, defined at /gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py:279> exception=CommClosedError()>
Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 288, in _
    handshake = await asyncio.wait_for(comm.read(), 1)
  File "/usr/lib64/python3.6/asyncio/tasks.py", line 351, in wait_for
    yield from waiter
concurrent.futures._base.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 295, in _
    raise CommClosedError()
distributed.comm.core.CommClosedError
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
Task exception was never retrieved
future: <Task finished coro=<connect.<locals>._() done, defined at /gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py:279> exception=CommClosedError()>
Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 288, in _
    handshake = await asyncio.wait_for(comm.read(), 1)
  File "/usr/lib64/python3.6/asyncio/tasks.py", line 351, in wait_for
    yield from waiter
concurrent.futures._base.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 295, in _
    raise CommClosedError()
distributed.comm.core.CommClosedError
Task exception was never retrieved
future: <Task finished coro=<connect.<locals>._() done, defined at /gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py:279> exception=CommClosedError()>
Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 288, in _
    handshake = await asyncio.wait_for(comm.read(), 1)
  File "/usr/lib64/python3.6/asyncio/tasks.py", line 351, in wait_for
    yield from waiter
concurrent.futures._base.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 295, in _
    raise CommClosedError()
distributed.comm.core.CommClosedError
Task exception was never retrieved
future: <Task finished coro=<connect.<locals>._() done, defined at /gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py:279> exception=CommClosedError()>
Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 288, in _
    handshake = await asyncio.wait_for(comm.read(), 1)
  File "/usr/lib64/python3.6/asyncio/tasks.py", line 351, in wait_for
    yield from waiter
concurrent.futures._base.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 295, in _
    raise CommClosedError()
distributed.comm.core.CommClosedError
Task exception was never retrieved
future: <Task finished coro=<connect.<locals>._() done, defined at /gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py:279> exception=CommClosedError()>
Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 288, in _
    handshake = await asyncio.wait_for(comm.read(), 1)
  File "/usr/lib64/python3.6/asyncio/tasks.py", line 351, in wait_for
    yield from waiter
concurrent.futures._base.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 295, in _
    raise CommClosedError()
distributed.comm.core.CommClosedError
Task exception was never retrieved
future: <Task finished coro=<connect.<locals>._() done, defined at /gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py:279> exception=CommClosedError()>
Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 288, in _
    handshake = await asyncio.wait_for(comm.read(), 1)
  File "/usr/lib64/python3.6/asyncio/tasks.py", line 351, in wait_for
    yield from waiter
concurrent.futures._base.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 295, in _
    raise CommClosedError()
distributed.comm.core.CommClosedError
Task exception was never retrieved
future: <Task finished coro=<connect.<locals>._() done, defined at /gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py:279> exception=CommClosedError()>
Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 288, in _
    handshake = await asyncio.wait_for(comm.read(), 1)
  File "/usr/lib64/python3.6/asyncio/tasks.py", line 351, in wait_for
    yield from waiter
concurrent.futures._base.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 295, in _
    raise CommClosedError()
distributed.comm.core.CommClosedError
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
Task exception was never retrieved
future: <Task finished coro=<connect.<locals>._() done, defined at /gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py:279> exception=CommClosedError()>
Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 288, in _
    handshake = await asyncio.wait_for(comm.read(), 1)
  File "/usr/lib64/python3.6/asyncio/tasks.py", line 351, in wait_for
    yield from waiter
concurrent.futures._base.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 295, in _
    raise CommClosedError()
distributed.comm.core.CommClosedError
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
Task exception was never retrieved
future: <Task finished coro=<connect.<locals>._() done, defined at /gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py:279> exception=CommClosedError()>
Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 288, in _
    handshake = await asyncio.wait_for(comm.read(), 1)
  File "/usr/lib64/python3.6/asyncio/tasks.py", line 351, in wait_for
    yield from waiter
concurrent.futures._base.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 295, in _
    raise CommClosedError()
distributed.comm.core.CommClosedError
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
Task exception was never retrieved
future: <Task finished coro=<connect.<locals>._() done, defined at /gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py:279> exception=CommClosedError()>
Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 288, in _
    handshake = await asyncio.wait_for(comm.read(), 1)
  File "/usr/lib64/python3.6/asyncio/tasks.py", line 351, in wait_for
    yield from waiter
concurrent.futures._base.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 295, in _
    raise CommClosedError()
distributed.comm.core.CommClosedError
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
Task exception was never retrieved
future: <Task finished coro=<connect.<locals>._() done, defined at /gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py:279> exception=CommClosedError()>
Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 288, in _
    handshake = await asyncio.wait_for(comm.read(), 1)
  File "/usr/lib64/python3.6/asyncio/tasks.py", line 351, in wait_for
    yield from waiter
concurrent.futures._base.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 295, in _
    raise CommClosedError()
distributed.comm.core.CommClosedError
Task exception was never retrieved
future: <Task finished coro=<connect.<locals>._() done, defined at /gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py:279> exception=CommClosedError()>
Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 288, in _
    handshake = await asyncio.wait_for(comm.read(), 1)
  File "/usr/lib64/python3.6/asyncio/tasks.py", line 351, in wait_for
    yield from waiter
concurrent.futures._base.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 295, in _
    raise CommClosedError()
distributed.comm.core.CommClosedError
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
Task exception was never retrieved
future: <Task finished coro=<connect.<locals>._() done, defined at /gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py:279> exception=CommClosedError()>
Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 288, in _
    handshake = await asyncio.wait_for(comm.read(), 1)
  File "/usr/lib64/python3.6/asyncio/tasks.py", line 351, in wait_for
    yield from waiter
concurrent.futures._base.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 295, in _
    raise CommClosedError()
distributed.comm.core.CommClosedError
Task exception was never retrieved
future: <Task finished coro=<connect.<locals>._() done, defined at /gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py:279> exception=CommClosedError()>
Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 288, in _
    handshake = await asyncio.wait_for(comm.read(), 1)
  File "/usr/lib64/python3.6/asyncio/tasks.py", line 351, in wait_for
    yield from waiter
concurrent.futures._base.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 295, in _
    raise CommClosedError()
distributed.comm.core.CommClosedError
Task exception was never retrieved
future: <Task finished coro=<connect.<locals>._() done, defined at /gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py:279> exception=CommClosedError()>
Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 288, in _
    handshake = await asyncio.wait_for(comm.read(), 1)
  File "/usr/lib64/python3.6/asyncio/tasks.py", line 351, in wait_for
    yield from waiter
concurrent.futures._base.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 295, in _
    raise CommClosedError()
distributed.comm.core.CommClosedError
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
Task exception was never retrieved
future: <Task finished coro=<connect.<locals>._() done, defined at /gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py:279> exception=CommClosedError()>
Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 288, in _
    handshake = await asyncio.wait_for(comm.read(), 1)
  File "/usr/lib64/python3.6/asyncio/tasks.py", line 351, in wait_for
    yield from waiter
concurrent.futures._base.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 295, in _
    raise CommClosedError()
distributed.comm.core.CommClosedError
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
Task exception was never retrieved
future: <Task finished coro=<connect.<locals>._() done, defined at /gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py:279> exception=CommClosedError()>
Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 288, in _
    handshake = await asyncio.wait_for(comm.read(), 1)
  File "/usr/lib64/python3.6/asyncio/tasks.py", line 351, in wait_for
    yield from waiter
concurrent.futures._base.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 295, in _
    raise CommClosedError()
distributed.comm.core.CommClosedError
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
Task exception was never retrieved
future: <Task finished coro=<connect.<locals>._() done, defined at /gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py:279> exception=CommClosedError()>
Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 288, in _
    handshake = await asyncio.wait_for(comm.read(), 1)
  File "/usr/lib64/python3.6/asyncio/tasks.py", line 351, in wait_for
    yield from waiter
concurrent.futures._base.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 295, in _
    raise CommClosedError()
distributed.comm.core.CommClosedError
Task exception was never retrieved
future: <Task finished coro=<connect.<locals>._() done, defined at /gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py:279> exception=CommClosedError()>
Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 288, in _
    handshake = await asyncio.wait_for(comm.read(), 1)
  File "/usr/lib64/python3.6/asyncio/tasks.py", line 351, in wait_for
    yield from waiter
concurrent.futures._base.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 295, in _
    raise CommClosedError()
distributed.comm.core.CommClosedError
Task exception was never retrieved
future: <Task finished coro=<connect.<locals>._() done, defined at /gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py:279> exception=CommClosedError()>
Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 288, in _
    handshake = await asyncio.wait_for(comm.read(), 1)
  File "/usr/lib64/python3.6/asyncio/tasks.py", line 351, in wait_for
    yield from waiter
concurrent.futures._base.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 295, in _
    raise CommClosedError()
distributed.comm.core.CommClosedError
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
Task exception was never retrieved
future: <Task finished coro=<connect.<locals>._() done, defined at /gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py:279> exception=CommClosedError()>
Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 288, in _
    handshake = await asyncio.wait_for(comm.read(), 1)
  File "/usr/lib64/python3.6/asyncio/tasks.py", line 351, in wait_for
    yield from waiter
concurrent.futures._base.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 295, in _
    raise CommClosedError()
distributed.comm.core.CommClosedError
Task exception was never retrieved
future: <Task finished coro=<connect.<locals>._() done, defined at /gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py:279> exception=CommClosedError()>
Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 288, in _
    handshake = await asyncio.wait_for(comm.read(), 1)
  File "/usr/lib64/python3.6/asyncio/tasks.py", line 351, in wait_for
    yield from waiter
concurrent.futures._base.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 295, in _
    raise CommClosedError()
distributed.comm.core.CommClosedError
Task exception was never retrieved
future: <Task finished coro=<connect.<locals>._() done, defined at /gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py:279> exception=CommClosedError()>
Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 288, in _
    handshake = await asyncio.wait_for(comm.read(), 1)
  File "/usr/lib64/python3.6/asyncio/tasks.py", line 351, in wait_for
    yield from waiter
concurrent.futures._base.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 295, in _
    raise CommClosedError()
distributed.comm.core.CommClosedError
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
Task exception was never retrieved
future: <Task finished coro=<connect.<locals>._() done, defined at /gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py:279> exception=CommClosedError()>
Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 288, in _
    handshake = await asyncio.wait_for(comm.read(), 1)
  File "/usr/lib64/python3.6/asyncio/tasks.py", line 351, in wait_for
    yield from waiter
concurrent.futures._base.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 295, in _
    raise CommClosedError()
distributed.comm.core.CommClosedError
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
Task exception was never retrieved
future: <Task finished coro=<connect.<locals>._() done, defined at /gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py:279> exception=CommClosedError()>
Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 288, in _
    handshake = await asyncio.wait_for(comm.read(), 1)
  File "/usr/lib64/python3.6/asyncio/tasks.py", line 351, in wait_for
    yield from waiter
concurrent.futures._base.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 295, in _
    raise CommClosedError()
distributed.comm.core.CommClosedError
Task exception was never retrieved
future: <Task finished coro=<connect.<locals>._() done, defined at /gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py:279> exception=CommClosedError()>
Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 288, in _
    handshake = await asyncio.wait_for(comm.read(), 1)
  File "/usr/lib64/python3.6/asyncio/tasks.py", line 351, in wait_for
    yield from waiter
concurrent.futures._base.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 295, in _
    raise CommClosedError()
distributed.comm.core.CommClosedError
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
Task exception was never retrieved
future: <Task finished coro=<connect.<locals>._() done, defined at /gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py:279> exception=CommClosedError()>
Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 288, in _
    handshake = await asyncio.wait_for(comm.read(), 1)
  File "/usr/lib64/python3.6/asyncio/tasks.py", line 351, in wait_for
    yield from waiter
concurrent.futures._base.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 295, in _
    raise CommClosedError()
distributed.comm.core.CommClosedError
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
Task exception was never retrieved
future: <Task finished coro=<connect.<locals>._() done, defined at /gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py:279> exception=CommClosedError()>
Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 288, in _
    handshake = await asyncio.wait_for(comm.read(), 1)
  File "/usr/lib64/python3.6/asyncio/tasks.py", line 351, in wait_for
    yield from waiter
concurrent.futures._base.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 295, in _
    raise CommClosedError()
distributed.comm.core.CommClosedError
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
Task exception was never retrieved
future: <Task finished coro=<connect.<locals>._() done, defined at /gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py:279> exception=CommClosedError()>
Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 288, in _
    handshake = await asyncio.wait_for(comm.read(), 1)
  File "/usr/lib64/python3.6/asyncio/tasks.py", line 351, in wait_for
    yield from waiter
concurrent.futures._base.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 295, in _
    raise CommClosedError()
distributed.comm.core.CommClosedError
Task exception was never retrieved
future: <Task finished coro=<connect.<locals>._() done, defined at /gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py:279> exception=CommClosedError()>
Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 288, in _
    handshake = await asyncio.wait_for(comm.read(), 1)
  File "/usr/lib64/python3.6/asyncio/tasks.py", line 351, in wait_for
    yield from waiter
concurrent.futures._base.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 295, in _
    raise CommClosedError()
distributed.comm.core.CommClosedError
Task exception was never retrieved
future: <Task finished coro=<connect.<locals>._() done, defined at /gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py:279> exception=CommClosedError()>
Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 288, in _
    handshake = await asyncio.wait_for(comm.read(), 1)
  File "/usr/lib64/python3.6/asyncio/tasks.py", line 351, in wait_for
    yield from waiter
concurrent.futures._base.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 295, in _
    raise CommClosedError()
distributed.comm.core.CommClosedError
Task exception was never retrieved
future: <Task finished coro=<connect.<locals>._() done, defined at /gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py:279> exception=CommClosedError()>
Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 288, in _
    handshake = await asyncio.wait_for(comm.read(), 1)
  File "/usr/lib64/python3.6/asyncio/tasks.py", line 351, in wait_for
    yield from waiter
concurrent.futures._base.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 295, in _
    raise CommClosedError()
distributed.comm.core.CommClosedError
Task exception was never retrieved
future: <Task finished coro=<connect.<locals>._() done, defined at /gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py:279> exception=CommClosedError()>
Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 288, in _
    handshake = await asyncio.wait_for(comm.read(), 1)
  File "/usr/lib64/python3.6/asyncio/tasks.py", line 351, in wait_for
    yield from waiter
concurrent.futures._base.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 295, in _
    raise CommClosedError()
distributed.comm.core.CommClosedError
Task exception was never retrieved
future: <Task finished coro=<connect.<locals>._() done, defined at /gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py:279> exception=CommClosedError()>
Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 288, in _
    handshake = await asyncio.wait_for(comm.read(), 1)
  File "/usr/lib64/python3.6/asyncio/tasks.py", line 351, in wait_for
    yield from waiter
concurrent.futures._base.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 295, in _
    raise CommClosedError()
distributed.comm.core.CommClosedError
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
Task exception was never retrieved
future: <Task finished coro=<connect.<locals>._() done, defined at /gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py:279> exception=CommClosedError()>
Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 288, in _
    handshake = await asyncio.wait_for(comm.read(), 1)
  File "/usr/lib64/python3.6/asyncio/tasks.py", line 351, in wait_for
    yield from waiter
concurrent.futures._base.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 295, in _
    raise CommClosedError()
distributed.comm.core.CommClosedError
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
Task exception was never retrieved
future: <Task finished coro=<connect.<locals>._() done, defined at /gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py:279> exception=CommClosedError()>
Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 288, in _
    handshake = await asyncio.wait_for(comm.read(), 1)
  File "/usr/lib64/python3.6/asyncio/tasks.py", line 351, in wait_for
    yield from waiter
concurrent.futures._base.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 295, in _
    raise CommClosedError()
distributed.comm.core.CommClosedError
Task exception was never retrieved
future: <Task finished coro=<connect.<locals>._() done, defined at /gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py:279> exception=CommClosedError()>
Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 288, in _
    handshake = await asyncio.wait_for(comm.read(), 1)
  File "/usr/lib64/python3.6/asyncio/tasks.py", line 351, in wait_for
    yield from waiter
concurrent.futures._base.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 295, in _
    raise CommClosedError()
distributed.comm.core.CommClosedError
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
Task exception was never retrieved
future: <Task finished coro=<connect.<locals>._() done, defined at /gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py:279> exception=CommClosedError()>
Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 288, in _
    handshake = await asyncio.wait_for(comm.read(), 1)
  File "/usr/lib64/python3.6/asyncio/tasks.py", line 351, in wait_for
    yield from waiter
concurrent.futures._base.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 295, in _
    raise CommClosedError()
distributed.comm.core.CommClosedError
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
Task exception was never retrieved
future: <Task finished coro=<connect.<locals>._() done, defined at /gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py:279> exception=CommClosedError()>
Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 288, in _
    handshake = await asyncio.wait_for(comm.read(), 1)
  File "/usr/lib64/python3.6/asyncio/tasks.py", line 351, in wait_for
    yield from waiter
concurrent.futures._base.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 295, in _
    raise CommClosedError()
distributed.comm.core.CommClosedError
Task exception was never retrieved
future: <Task finished coro=<connect.<locals>._() done, defined at /gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py:279> exception=CommClosedError()>
Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 288, in _
    handshake = await asyncio.wait_for(comm.read(), 1)
  File "/usr/lib64/python3.6/asyncio/tasks.py", line 351, in wait_for
    yield from waiter
concurrent.futures._base.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 295, in _
    raise CommClosedError()
distributed.comm.core.CommClosedError
Task exception was never retrieved
future: <Task finished coro=<connect.<locals>._() done, defined at /gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py:279> exception=CommClosedError()>
Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 288, in _
    handshake = await asyncio.wait_for(comm.read(), 1)
  File "/usr/lib64/python3.6/asyncio/tasks.py", line 351, in wait_for
    yield from waiter
concurrent.futures._base.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 295, in _
    raise CommClosedError()
distributed.comm.core.CommClosedError
Task exception was never retrieved
future: <Task finished coro=<connect.<locals>._() done, defined at /gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py:279> exception=CommClosedError()>
Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 288, in _
    handshake = await asyncio.wait_for(comm.read(), 1)
  File "/usr/lib64/python3.6/asyncio/tasks.py", line 351, in wait_for
    yield from waiter
concurrent.futures._base.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 295, in _
    raise CommClosedError()
distributed.comm.core.CommClosedError
Task exception was never retrieved
future: <Task finished coro=<connect.<locals>._() done, defined at /gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py:279> exception=CommClosedError()>
Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 288, in _
    handshake = await asyncio.wait_for(comm.read(), 1)
  File "/usr/lib64/python3.6/asyncio/tasks.py", line 351, in wait_for
    yield from waiter
concurrent.futures._base.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 295, in _
    raise CommClosedError()
distributed.comm.core.CommClosedError
Task exception was never retrieved
future: <Task finished coro=<connect.<locals>._() done, defined at /gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py:279> exception=CommClosedError()>
Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 288, in _
    handshake = await asyncio.wait_for(comm.read(), 1)
  File "/usr/lib64/python3.6/asyncio/tasks.py", line 351, in wait_for
    yield from waiter
concurrent.futures._base.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 295, in _
    raise CommClosedError()
distributed.comm.core.CommClosedError
Task exception was never retrieved
future: <Task finished coro=<connect.<locals>._() done, defined at /gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py:279> exception=CommClosedError()>
Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 288, in _
    handshake = await asyncio.wait_for(comm.read(), 1)
  File "/usr/lib64/python3.6/asyncio/tasks.py", line 351, in wait_for
    yield from waiter
concurrent.futures._base.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 295, in _
    raise CommClosedError()
distributed.comm.core.CommClosedError
Task exception was never retrieved
future: <Task finished coro=<connect.<locals>._() done, defined at /gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py:279> exception=CommClosedError()>
Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 288, in _
    handshake = await asyncio.wait_for(comm.read(), 1)
  File "/usr/lib64/python3.6/asyncio/tasks.py", line 351, in wait_for
    yield from waiter
concurrent.futures._base.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 295, in _
    raise CommClosedError()
distributed.comm.core.CommClosedError
Task exception was never retrieved
future: <Task finished coro=<connect.<locals>._() done, defined at /gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py:279> exception=CommClosedError()>
Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 288, in _
    handshake = await asyncio.wait_for(comm.read(), 1)
  File "/usr/lib64/python3.6/asyncio/tasks.py", line 351, in wait_for
    yield from waiter
concurrent.futures._base.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 295, in _
    raise CommClosedError()
distributed.comm.core.CommClosedError
Task exception was never retrieved
future: <Task finished coro=<connect.<locals>._() done, defined at /gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py:279> exception=CommClosedError()>
Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 288, in _
    handshake = await asyncio.wait_for(comm.read(), 1)
  File "/usr/lib64/python3.6/asyncio/tasks.py", line 351, in wait_for
    yield from waiter
concurrent.futures._base.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 295, in _
    raise CommClosedError()
distributed.comm.core.CommClosedError
Task exception was never retrieved
future: <Task finished coro=<connect.<locals>._() done, defined at /gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py:279> exception=CommClosedError()>
Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 288, in _
    handshake = await asyncio.wait_for(comm.read(), 1)
  File "/usr/lib64/python3.6/asyncio/tasks.py", line 351, in wait_for
    yield from waiter
concurrent.futures._base.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 295, in _
    raise CommClosedError()
distributed.comm.core.CommClosedError
Task exception was never retrieved
future: <Task finished coro=<connect.<locals>._() done, defined at /gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py:279> exception=CommClosedError()>
Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 288, in _
    handshake = await asyncio.wait_for(comm.read(), 1)
  File "/usr/lib64/python3.6/asyncio/tasks.py", line 351, in wait_for
    yield from waiter
concurrent.futures._base.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 295, in _
    raise CommClosedError()
distributed.comm.core.CommClosedError
Task exception was never retrieved
future: <Task finished coro=<connect.<locals>._() done, defined at /gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py:279> exception=CommClosedError()>
Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 288, in _
    handshake = await asyncio.wait_for(comm.read(), 1)
  File "/usr/lib64/python3.6/asyncio/tasks.py", line 351, in wait_for
    yield from waiter
concurrent.futures._base.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 295, in _
    raise CommClosedError()
distributed.comm.core.CommClosedError
Task exception was never retrieved
future: <Task finished coro=<connect.<locals>._() done, defined at /gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py:279> exception=CommClosedError()>
Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 288, in _
    handshake = await asyncio.wait_for(comm.read(), 1)
  File "/usr/lib64/python3.6/asyncio/tasks.py", line 351, in wait_for
    yield from waiter
concurrent.futures._base.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 295, in _
    raise CommClosedError()
distributed.comm.core.CommClosedError
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
Task exception was never retrieved
future: <Task finished coro=<connect.<locals>._() done, defined at /gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py:279> exception=CommClosedError()>
Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 288, in _
    handshake = await asyncio.wait_for(comm.read(), 1)
  File "/usr/lib64/python3.6/asyncio/tasks.py", line 351, in wait_for
    yield from waiter
concurrent.futures._base.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 295, in _
    raise CommClosedError()
distributed.comm.core.CommClosedError
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
Task exception was never retrieved
future: <Task finished coro=<connect.<locals>._() done, defined at /gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py:279> exception=CommClosedError()>
Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 288, in _
    handshake = await asyncio.wait_for(comm.read(), 1)
  File "/usr/lib64/python3.6/asyncio/tasks.py", line 351, in wait_for
    yield from waiter
concurrent.futures._base.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 295, in _
    raise CommClosedError()
distributed.comm.core.CommClosedError
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
Task exception was never retrieved
future: <Task finished coro=<connect.<locals>._() done, defined at /gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py:279> exception=CommClosedError()>
Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 288, in _
    handshake = await asyncio.wait_for(comm.read(), 1)
  File "/usr/lib64/python3.6/asyncio/tasks.py", line 351, in wait_for
    yield from waiter
concurrent.futures._base.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 295, in _
    raise CommClosedError()
distributed.comm.core.CommClosedError
Task exception was never retrieved
future: <Task finished coro=<connect.<locals>._() done, defined at /gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py:279> exception=CommClosedError()>
Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 288, in _
    handshake = await asyncio.wait_for(comm.read(), 1)
  File "/usr/lib64/python3.6/asyncio/tasks.py", line 351, in wait_for
    yield from waiter
concurrent.futures._base.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 295, in _
    raise CommClosedError()
distributed.comm.core.CommClosedError
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
Task exception was never retrieved
future: <Task finished coro=<connect.<locals>._() done, defined at /gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py:279> exception=CommClosedError()>
Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 288, in _
    handshake = await asyncio.wait_for(comm.read(), 1)
  File "/usr/lib64/python3.6/asyncio/tasks.py", line 351, in wait_for
    yield from waiter
concurrent.futures._base.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 295, in _
    raise CommClosedError()
distributed.comm.core.CommClosedError
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
Task exception was never retrieved
future: <Task finished coro=<connect.<locals>._() done, defined at /gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py:279> exception=CommClosedError()>
Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 288, in _
    handshake = await asyncio.wait_for(comm.read(), 1)
  File "/usr/lib64/python3.6/asyncio/tasks.py", line 351, in wait_for
    yield from waiter
concurrent.futures._base.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gpfs/exfel/data/scratch/roscar/work/midtools/midtools/.venv/lib64/python3.6/site-packages/distributed/comm/core.py", line 295, in _
    raise CommClosedError()
distributed.comm.core.CommClosedError
distributed.worker - INFO -         Registered to:   tcp://10.255.32.50:36178
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.255.32.50:36178
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.255.32.50:36178
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.255.32.50:36178
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.255.32.50:36178
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.255.32.50:36178
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.255.32.50:36178
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.255.32.50:36178
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.worker - INFO - Stopping worker at tcp://10.255.34.151:32927
distributed.worker - INFO - Stopping worker at tcp://10.255.34.151:33841
distributed.worker - INFO - Stopping worker at tcp://10.255.34.151:34382
distributed.worker - INFO - Stopping worker at tcp://10.255.34.151:35910
distributed.worker - INFO - Stopping worker at tcp://10.255.34.151:36769
distributed.worker - INFO - Stopping worker at tcp://10.255.34.151:34969
distributed.worker - INFO - Stopping worker at tcp://10.255.34.151:37984
distributed.worker - INFO - Stopping worker at tcp://10.255.34.151:43460
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - WARNING - Restarting worker
distributed.nanny - WARNING - Restarting worker
distributed.worker - INFO -       Start worker at:  tcp://10.255.34.151:43680
distributed.worker - INFO -          Listening to:  tcp://10.255.34.151:43680
distributed.worker - INFO -          dashboard at:        10.255.34.151:44780
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   96.00 GB
distributed.worker - INFO -       Local Directory: /scratch/dask-worker-space/worker-u2if_ci_
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://10.255.34.151:45073
distributed.worker - INFO -          Listening to:  tcp://10.255.34.151:45073
distributed.worker - INFO -          dashboard at:        10.255.34.151:40907
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   96.00 GB
distributed.worker - INFO -       Local Directory: /scratch/dask-worker-space/worker-iq0lzfir
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://10.255.32.50:36178
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.255.32.50:36178
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:  tcp://10.255.34.151:39752
distributed.worker - INFO -          Listening to:  tcp://10.255.34.151:39752
distributed.worker - INFO -          dashboard at:        10.255.34.151:43672
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   96.00 GB
distributed.worker - INFO -       Local Directory: /scratch/dask-worker-space/worker-7jtt6d7m
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://10.255.34.151:44596
distributed.worker - INFO -          Listening to:  tcp://10.255.34.151:44596
distributed.worker - INFO -          dashboard at:        10.255.34.151:40397
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   96.00 GB
distributed.worker - INFO -       Local Directory: /scratch/dask-worker-space/worker-zb7y8v4l
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://10.255.32.50:36178
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.255.32.50:36178
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:  tcp://10.255.34.151:34961
distributed.worker - INFO -          Listening to:  tcp://10.255.34.151:34961
distributed.worker - INFO -          dashboard at:        10.255.34.151:42377
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   96.00 GB
distributed.worker - INFO -       Local Directory: /scratch/dask-worker-space/worker-v_aauhqt
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://10.255.32.50:36178
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:  tcp://10.255.34.151:38342
distributed.worker - INFO -          Listening to:  tcp://10.255.34.151:38342
distributed.worker - INFO -          dashboard at:        10.255.34.151:45753
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   96.00 GB
distributed.worker - INFO -       Local Directory: /scratch/dask-worker-space/worker-cs4vhuqz
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://10.255.34.151:37751
distributed.worker - INFO -          Listening to:  tcp://10.255.34.151:37751
distributed.worker - INFO -          dashboard at:        10.255.34.151:42113
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   96.00 GB
distributed.worker - INFO -       Local Directory: /scratch/dask-worker-space/worker-1lzw9rvm
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://10.255.34.151:34119
distributed.worker - INFO -          Listening to:  tcp://10.255.34.151:34119
distributed.worker - INFO -          dashboard at:        10.255.34.151:38579
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   96.00 GB
distributed.worker - INFO -       Local Directory: /scratch/dask-worker-space/worker-f7tjxj3s
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://10.255.32.50:36178
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.255.32.50:36178
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.255.32.50:36178
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.comm.tcp - INFO - Connection closed before handshake completed
distributed.worker - INFO - Stopping worker at tcp://10.255.34.151:34119
distributed.worker - INFO - Stopping worker at tcp://10.255.34.151:37751
distributed.worker - INFO - Stopping worker at tcp://10.255.34.151:34961
distributed.worker - INFO - Stopping worker at tcp://10.255.34.151:38342
distributed.worker - INFO - Stopping worker at tcp://10.255.34.151:39752
distributed.worker - INFO - Stopping worker at tcp://10.255.34.151:45073
distributed.worker - INFO - Stopping worker at tcp://10.255.34.151:44596
distributed.worker - INFO - Stopping worker at tcp://10.255.34.151:43680
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - WARNING - Restarting worker
distributed.worker - INFO -       Start worker at:  tcp://10.255.34.151:40015
distributed.worker - INFO -          Listening to:  tcp://10.255.34.151:40015
distributed.worker - INFO -          dashboard at:        10.255.34.151:33796
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   96.00 GB
distributed.worker - INFO -       Local Directory: /scratch/dask-worker-space/worker-1w2h7vv5
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://10.255.32.50:36178
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:  tcp://10.255.34.151:45101
distributed.worker - INFO -          Listening to:  tcp://10.255.34.151:45101
distributed.worker - INFO -          dashboard at:        10.255.34.151:33739
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   96.00 GB
distributed.worker - INFO -       Local Directory: /scratch/dask-worker-space/worker-n3g4yd8g
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://10.255.34.151:43317
distributed.worker - INFO -          Listening to:  tcp://10.255.34.151:43317
distributed.worker - INFO -          dashboard at:        10.255.34.151:40221
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   96.00 GB
distributed.worker - INFO -       Local Directory: /scratch/dask-worker-space/worker-of9y3fkp
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://10.255.32.50:36178
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.255.32.50:36178
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:  tcp://10.255.34.151:42678
distributed.worker - INFO -          Listening to:  tcp://10.255.34.151:42678
distributed.worker - INFO -          dashboard at:        10.255.34.151:45902
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   96.00 GB
distributed.worker - INFO -       Local Directory: /scratch/dask-worker-space/worker-tmuc00rh
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://10.255.34.151:33904
distributed.worker - INFO -          Listening to:  tcp://10.255.34.151:33904
distributed.worker - INFO -          dashboard at:        10.255.34.151:33913
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   96.00 GB
distributed.worker - INFO -       Local Directory: /scratch/dask-worker-space/worker-ipav39sb
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://10.255.34.151:36399
distributed.worker - INFO -          Listening to:  tcp://10.255.34.151:36399
distributed.worker - INFO -          dashboard at:        10.255.34.151:34072
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   96.00 GB
distributed.worker - INFO -       Local Directory: /scratch/dask-worker-space/worker-2ri62eb8
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://10.255.34.151:34900
distributed.worker - INFO -          Listening to:  tcp://10.255.34.151:34900
distributed.worker - INFO -          dashboard at:        10.255.34.151:36661
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   96.00 GB
distributed.worker - INFO -       Local Directory: /scratch/dask-worker-space/worker-fzhz_h_e
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://10.255.32.50:36178
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.255.32.50:36178
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.255.32.50:36178
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.255.32.50:36178
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:  tcp://10.255.34.151:42915
distributed.worker - INFO -          Listening to:  tcp://10.255.34.151:42915
distributed.worker - INFO -          dashboard at:        10.255.34.151:41228
distributed.worker - INFO - Waiting to connect to:   tcp://10.255.32.50:36178
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   96.00 GB
distributed.worker - INFO -       Local Directory: /scratch/dask-worker-space/worker-pvxgxanz
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://10.255.32.50:36178
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
slurmstepd: error: *** JOB 5318341 ON max-exfl225 CANCELLED AT 2020-08-19T17:25:21 ***
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://10.255.34.151:46243'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.255.34.151:38024'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.255.34.151:46379'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.255.34.151:45786'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.255.34.151:35774'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.255.34.151:46482'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.255.34.151:33263'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.255.34.151:45741'
distributed.nanny - INFO - Worker process 196645 was killed by signal 15
distributed.nanny - INFO - Worker process 196633 was killed by signal 15
distributed.nanny - INFO - Worker process 196637 was killed by signal 15
distributed.nanny - INFO - Worker process 196639 was killed by signal 15
distributed.nanny - INFO - Worker process 196642 was killed by signal 15
distributed.nanny - INFO - Worker process 196630 was killed by signal 15
